---
title: "simulation"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(cache = TRUE)

library(knitr)
library(MASS)
library(RColorBrewer)

euclidean2 <- function(x, y) sum((x - y)^2)  ## squared euclidean distance

n_sim <- 1E5  ## number replicates
V <- 100  ## number voxels

rf <- colorRampPalette(rev(brewer.pal(11, 'Spectral')))  ## for palette
r <- rf(32)

```

# definitions

## mutivariate and univariate distance measures

Let $x_{vs}$ be the b-coefficient estimate for vertex $v \in 1, \ldots, V$ vertices and for subject $s \in 1, \ldots, S$, from run 1 of the experiment. 
Let $y_{vt}$ be an analogous estimate from run 2 of the experiment, for a not-necessarily-different subject $t \in 1, \ldots, S$.
These estimates are elements of matrices $\mathbf{X}$ and $\mathbf{Y}$, both of size $V\times S$.

**Multivariate distance measure**: the squared Euclidean distance between vectors $x_s$ and $y_t$.

\[\mathit{m}_{st} = \frac{1}{V}\sum_{v=1}^{V} (x_{vs} - y_{vt})^2\]

We scale by the number of vertices $V$ to aid comparability across regions of interest.

**Univariate distance measure**: the squared Euclidean distance between the sum of the elements of $x_s$ and of $y_t$.

\[\mathit{u}_{st} = \frac{1}{V}(\sum_{v=1}^{V}x_{vs} - \sum_{v=1}^{V}y_{vt})^2\]

Because the sum of the elements of a vector is a scalar, here, the Euclidean distance simplifies to the squared difference between the terms.


## intersubject discrimination index (IDI)

These across-run multivariate distances, $\mathit{m}_{st}$, can be collated into the $S\times S$ cross-run distance matrix $\mathbf{M}$.
This matrix is asymmetric, with rows corresponding to run 1, and columns corresponding to run 2. 
The diagonal elements contain within-subject distances, while the off-diagonal elements contain between-subject distances.

Analogously, these cross-run univariate distances, $\mathit{u}_{st}$ can be collated into an $S\times S$ matrix $\mathbf{U}$.


The IDI is computed via a contrast on these cross-run distance matrices.
The IDI is defined as the mean of the off-diagonal elements minus the mean of the diagonal elements.
Formally, for an arbitrary $S\times S$ cross-run distance matrix $\mathbf{D}$,


\[\mathit{idi}(\mathbf{D}_{S\times S}) = 
\frac{\sum_{s=1,t>s}^{S}d_{st} + \sum_{s>t,t=1}^{S}d_{st}}{S^2-S} - \frac{tr(\mathbf{D})}{S}\]


The multivariate and univariate IDI statistics can now be defined:

\[\mathit{mIDI} = \mathit{idi}(\mathbf{M})\]

and

\[\mathit{uIDI} = \mathit{idi}(\mathbf{U})\]


# false positive simulation

```{r simulate}

d <- as.data.frame(matrix(NA, ncol = 2, nrow = n_sim))
names(d) <- c("m", "u")

for (ii in 1:n_sim) {

  x1 <- rnorm(V)
  x2 <- rnorm(V)
  
  d$m[ii] <- euclidean2(x1, x2) / V
  d$u[ii] <- euclidean2(sum(x1), sum(x2)) / V
  
}

```


## results

Expected values seem to be equivalent.
However, variance (and PDF) differs depending on V.

```{r}

colMeans(d)
apply(d, 2, var)
t.test(d$m, d$u, paired = TRUE)$statistic

hist(d$m)
hist(d$u)


kernel_d2 <- kde2d(d$m, d$u)
image(kernel_d2, col = r, main = "euclidean^2")

```

### square-root transform

Taking square root breaks equivalence of expected values.

```{r}

colMeans(sqrt(d))
t.test(sqrt(d$m), sqrt(d$u), paired = TRUE)$statistic

hist(sqrt(d$m))
hist(sqrt(d$u))
apply(sqrt(d), 2, var)

kernel_d <- kde2d(sqrt(d$m), sqrt(d$u))
image(kernel_d, col = r, main = "euclidean")

```

### comparison to Chi-square

These simulated distributions are well-matched by the Chi-square distributions, with *df* of either 1 (for univariate measure) or V (for multivariate measure).

```{r}

qqplot(
  qchisq(ppoints(n_sim), df = 1), 
  d$u, 
  main = expression("Q-Q plot for" ~~ {chi^2}[k == 1]),
  xlab = "theoretical",
  ylab = "univariate distances"
  )

qqplot(
  qchisq(ppoints(n_sim), df = V), 
  d$m, 
  main = expression("Q-Q plot for" ~~ {chi^2}[k == V]),
  xlab = "theoretical",
  ylab = "multivariate distances"
  )


```


# strong univariate effect, no multivariate effect

```{r}

d_univ <- as.data.frame(matrix(NA, ncol = 2, nrow = n_sim))
names(d_univ) <- c("m", "u")

for (ii in 1:n_sim) {

  x1 <- rnorm(V)
  x2 <- rnorm(V, mean = 10)
  
  d_univ$m[ii] <- euclidean2(x1, x2) / V
  d_univ$u[ii] <- euclidean2(sum(x1), sum(x2)) / V
  
}

```

## results


Now the univariate distance is much larger.

```{r}

colMeans(d_univ)
apply(d_univ, 2, var)
t.test(d_univ$m, d_univ$u, paired = TRUE)$statistic

hist(d_univ$m)
hist(d_univ$u)

kernel_d2_univ <- kde2d(d_univ$m, d_univ$u)
image(kernel_d2_univ, col = r, main = "euclidean^2")

```



# strong multivariate effect, no univariate effect

```{r}

d_multiv <- as.data.frame(matrix(NA, ncol = 2, nrow = n_sim))
names(d_multiv) <- c("m", "u")

for (ii in 1:n_sim) {

  x1 <- rnorm(V) + (V:1 - mean(1:V))/10  ## pattern across vertices, but across-voxel mean of 0
  x2 <- rnorm(V) + (1:V - mean(1:V))/10  ## opposite pattern
  
  d_multiv$m[ii] <- euclidean2(x1, x2) / V
  d_multiv$u[ii] <- euclidean2(sum(x1), sum(x2)) / V
  
}

```

## results


Now the multivariate distance is much larger.

```{r}

colMeans(d_multiv)
apply(d_multiv, 2, var)
t.test(d_multiv$m, d_multiv$u, paired = TRUE)$statistic

hist(d_multiv$m)
hist(d_multiv$u)

kernel_d2_multiv <- kde2d(d_multiv$m, d_multiv$u)
image(kernel_d2_multiv, col = r, main = "euclidean^2")

```


